{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uozGV6740aer"
   },
   "source": [
    "# Label Generation\n",
    "\n",
    "This notebook outlines the process for obtaining Labels from various LLMs, including their results and also includes the implementation of Llama-3.2-3B-Instruct for this experiment.\n",
    "\n",
    "**Note**: A Hugging Face user access token is needed for this project. Go to [this](https://huggingface.co/docs/hub/security-tokens) website to set up a Hugging Face Account if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjddeFKOQZab"
   },
   "source": [
    "# Models Chosen\n",
    "\n",
    "It was decided to test a range of LLMs. To ensure a fair comparison, only free versions of models were chosen, however an account of some description is needed to access each model. The Models chosen are as follows:\n",
    "* **[ChatGPT 4o](https://chatgpt.com/)**: The free version was used, however a free OpenAI account is needed.\n",
    "* **[Claude 3.7 Sonnet](https://claude.ai/login?returnTo=%2F%3F)**: Free but Claude account needed.\n",
    "* **[Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)**: Open-source, however, a free Hugging Face account and accesss key are needed to use this model.\n",
    "* **[DeepSeek DeepThink R1](https://chat.deepseek.com/sign_in)**: Used for Evaluation, free Deepseek account also needed.\n",
    "\n",
    "## Aim of LLM Usage\n",
    "As Topic Modeling is an unsupervised task, the results obtained from both LDA and BERTopic do not contain and actual labels that could be used to categorize the articles. The results are a list of key words to each topic and are included below. The aim of using LLMs is to generate fluent and practical labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpYugP1STn7r"
   },
   "source": [
    "## Results from LDA & BERTopic Topic Modeling Approaches\n",
    "Below are the unsupervised results which we would like to create labels for.\n",
    "\n",
    "###LDA\n",
    "```\n",
    "Topic_1: student, participant, survey, mental, online, anxiety, education, child, self, psychological\n",
    "Topic_2: vaccine, vaccination, antibody, variant, transmission, influenza, testing, assay, detection, detect\n",
    "Topic_3: mask, food, air, concentration, surface, energy, water, temperature, material, environmental\n",
    "Topic_4: policy, article, service, community, crisis, economic, global, management, paper, world\n",
    "Topic_5: ci, mortality, p, death, trial, child, admission, severity, cohort, covid\n",
    "Topic_6: protein, drug, immune, target, gene, expression, receptor, mechanism, induce, therapeutic\n",
    "Topic_7: cancer, surgery, lung, complication, surgical, pain, procedure, undergo, diagnosis, pulmonary\n",
    "Topic_8: propose, network, performance, learning, image, application, accuracy, user, predict, tool\n",
    "```\n",
    "\n",
    "###BERTopic\n",
    "```\n",
    "Topic_1: ['teacher', 'teaching', 'learning', 'learn', 'student', 'online', 'education', 'university', 'educational', 'learner']\n",
    "Topic_2: ['child', 'pediatric', 'multisystem', 'inflammatory', 'pim', 'syndrome', 'infant', 'adult', 'fever', 'ts']\n",
    "Topic_3: ['replication', 'glycan', 'protein', 'rna', 'virus', 'viral', 'membrane', 'glycosylation', 'virion', 'host']\n",
    "Topic_4: ['image', 'ray', 'deep', 'segmentation', 'chest', 'classification', 'dataset', 'convolutional', 'ct', 'learning']\n",
    "Topic_5: ['government', 'governance', 'law', 'policy', 'country', 'public', 'global', 'right', 'political', 'international']\n",
    "Topic_6: ['protease', 'docking', 'compound', 'inhibitor', 'mpro', 'drug', 'molecular', 'bind', 'ligand', 'pro']\n",
    "Topic_7: ['vte', 'thrombosis', 'anticoagulation', 'embolism', 'venous', 'coagulation', 'thrombotic', 'thromboembolism', 'dimer', 'heparin']\n",
    "Topic_8: ['hospital', 'hcw', 'bed', 'ambulance', 'staff', 'emergency', 'healthcare', 'preparedness', 'care', 'facility']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guFnJ4gyT5aV"
   },
   "source": [
    "## Prompt Design\n",
    "The following was the prompt used for ChatGPT, LLama3.2 and Claude. The example labels were taken from an article on [the New England Journal of Medicine](https://www.nejm.org/doi/10.1056/NEJMsb2408466). This was chosen as it is a related topic but as it was released last year, is not connected to the CORD-19 datset.\n",
    "```\n",
    "Please analyze these keyword groups and create a specific, descriptive label for each topic that:\n",
    "Captures the central theme with precision in the style of the example topics given.\n",
    "Uses domain-specific terminology appropriate for medical/scientific research\n",
    "Is concise (preferably 2-5 words).\n",
    "Format your response as a numbered list corresponding to the topic numbers provided.\n",
    "Example Labels: Cardiology General, Clinical Medicine General, Coronavirus, Critical Care, Diagnostics, Global Health, Hospital-Based Clinical Medicine, Infectious Disease General, Neurology/Neurosurgery General, Outpatient-Based Clinical Medicine, Public Health General, Viral Infections, Pulmonary/Critical Care General\n",
    "\n",
    "Input: [Topic Lists from LDA/BERTopic]\n",
    "\n",
    "Labels:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Rx8LO2iVS7j"
   },
   "source": [
    "#Results\n",
    "The generated labels from each model is included below.\n",
    "\n",
    "##ChatGPT 4o\n",
    "\n",
    "###LDA\n",
    "\n",
    "```\n",
    "Mental Health in Educational Settings\n",
    "Vaccine Immunology and Detection\n",
    "Environmental Exposure and Contamination\n",
    "Global Health Policy and Systems\n",
    "Pediatric COVID-19 Outcomes\n",
    "Molecular Therapeutics and Immunomodulation\n",
    "Thoracic Oncology and Surgical Outcomes\n",
    "AI Applications in Medical Imaging\n",
    "```\n",
    "###BERTopic\n",
    "```\n",
    "Health Professions Education\n",
    "Pediatric Inflammatory Syndromes\n",
    "Viral Molecular Biology\n",
    "Medical Imaging and AI\n",
    "Global Health Policy\n",
    "Antiviral Drug Discovery\n",
    "Venous Thromboembolism Management\n",
    "Hospital-Based Emergency Care\n",
    "```\n",
    "\n",
    "\n",
    "##Claude\n",
    "###LDA\n",
    "```\n",
    "Pediatric Mental Health Assessment\n",
    "Vaccine Immunology & Surveillance\n",
    "Environmental Exposure Determinants\n",
    "Global Health Policy Management\n",
    "Clinical Outcomes Epidemiology\n",
    "Molecular Immunotherapeutics\n",
    "Thoracic Surgical Oncology\n",
    "Predictive Diagnostic Algorithms\n",
    "```\n",
    "\n",
    "###BERTopic\n",
    "\n",
    "```\n",
    "Medical Education Pedagogy\n",
    "Pediatric Inflammatory Syndromes\n",
    "Viral Molecular Biology\n",
    "Medical Imaging Diagnostics\n",
    "Health Policy Governance\n",
    "Drug Discovery Proteomics\n",
    "Thrombotic Disease Management\n",
    "Healthcare Facility Operations\n",
    "```\n",
    "\n",
    "##Llama3.2\n",
    "###LDA\n",
    "\n",
    "```\n",
    "Child Mental Health Education Survey\n",
    "Influenza Vaccine Detection Assay\n",
    "Environmental Air Quality Assessment\n",
    "Global Economic Crisis Management Policy\n",
    "COVID-19 Child Mortality Severity Cohort\n",
    "Immune Targeted Therapeutic Protein Expression\n",
    "Pulmonary Cancer Surgery Diagnosis Complication\n",
    "Image-Based Learning Performance Prediction Tool\n",
    "```\n",
    "###BERTopic\n",
    "\n",
    "```\n",
    "Educational Technology\n",
    "Pediatric Multisystem Inflammatory Syndrome\n",
    "Viral Glycan Interactions\n",
    "Deep Learning Chest X-ray Segmentation\n",
    "Global Governance and International Law\n",
    "Protease Inhibitor Compound Design\n",
    "Venous Thrombosis and Coagulation Management\n",
    "Emergency Healthcare Preparedness and Staffing\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68WQ22BuWYf1"
   },
   "source": [
    "# DeepSeek Evaluation Using Reasoning Model\n",
    "As Human Evaluation was outside the scope of this project, the following prompt was given to the R1 Reasoning model of DeepSeek. The limitations of this approach are discussed further in the report.\n",
    "\n",
    "##Prompt##\n",
    "```\n",
    "We asked three LLMs to generate topic labels based on lists of words for a collection of medical/scientific articles. Each model created 8 topic labels based on 8 lists of words. Evaluate the performance of each model using the following metrics on a 1-5 scale, where 1 is poor performance and 5 high performance:\n",
    "Fluency/ Coherence, i.e. how fluent and grammatical is the language of the topic label?\n",
    "Suitability, i.e. how well does the label describe the words associated with it?\n",
    "Usability, i.e. is the topic label usable as a topic tag?\n",
    "Please also choose the best topic label for each list of words and mention if there are any topic labels you would change.\n",
    "Please explain all your answers.\n",
    "\n",
    "Lists of Words:\n",
    "[Topic Lists]\n",
    "\n",
    "Model 1:\n",
    "[Labels]\n",
    "\n",
    "Model 2:\n",
    "[Labels]\n",
    "\n",
    "Model 3:\n",
    "[Labels]\n",
    "```\n",
    "\n",
    "##Results\n",
    "###LDA\n",
    "```\n",
    "Pediatric Mental Health Assessment\n",
    "Vaccine Immunology & Surveillance\n",
    "Environmental Exposure and Contamination\n",
    "Global Economic Crisis Policy\n",
    "Pediatric COVID-19 Outcomes\n",
    "Molecular Therapeutics and Immunomodulation\n",
    "Thoracic Oncology and Surgical Outcomes\n",
    "AI-Driven Medical Imaging Tools\n",
    "```\n",
    "\n",
    "\n",
    "###BERTopic\n",
    "```\n",
    "Online Education and Learning\n",
    "Pediatric Multisystem Inflammatory Syndrome\n",
    "Viral Glycan Interactions\n",
    "Deep Learning Chest X-ray Segmentation\n",
    "Global Governance and International Law\n",
    "Protease Inhibitor Compound Design\n",
    "Venous Thromboembolism Management\n",
    "Emergency Healthcare Preparedness and Staffing\n",
    "```\n",
    "\n",
    "The next cell includes the full response and reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03MWfCPPYvkc"
   },
   "source": [
    "#DeepSeek Response - LDA\n",
    "### Model Evaluations\n",
    "\n",
    "#### **Model 1**  \n",
    "**Fluency/Coherence**: 4.8/5  \n",
    "**Suitability**: 4.4/5  \n",
    "**Usability**: 4.8/5  \n",
    "**Strengths**: Highly fluent and concise labels. Struggled only with Topic_4 (misinterpreted \"health\").  \n",
    "**Weaknesses**: Topic_4 inaccurately included \"health,\" reducing suitability.  \n",
    "\n",
    "#### **Model 2**  \n",
    "**Fluency/Coherence**: 4.6/5  \n",
    "**Suitability**: 4.5/5  \n",
    "**Usability**: 4.5/5  \n",
    "**Strengths**: Balanced specificity and fluency (e.g., \"Pediatric Mental Health Assessment\").  \n",
    "**Weaknesses**: Topic_4 still included \"health,\" and Topic_5 was overly vague.  \n",
    "\n",
    "#### **Model 3**  \n",
    "**Fluency/Coherence**: 3.1/5  \n",
    "**Suitability**: 4.1/5  \n",
    "**Usability**: 3.0/5  \n",
    "**Strengths**: High specificity (e.g., Topic_4, Topic_5).  \n",
    "**Weaknesses**: Clunky phrasing (e.g., \"COVID-19 Child Mortality Severity Cohort\").  \n",
    "\n",
    "---\n",
    "\n",
    "### **Best Labels per Topic**  \n",
    "1. **Topic_1**: *Pediatric Mental Health Assessment* (Model 2)  \n",
    "   - Covers \"child,\" \"mental health,\" and \"survey\" (as \"assessment\").  \n",
    "2. **Topic_2**: *Vaccine Immunology & Surveillance* (Model 2)  \n",
    "   - \"Surveillance\" captures variant tracking and testing.  \n",
    "3. **Topic_3**: *Environmental Exposure and Contamination* (Model 1)  \n",
    "   - Broadly covers air, water, and materials.  \n",
    "4. **Topic_4**: *Global Economic Crisis Management Policy* (Model 3)  \n",
    "   - Accurately reflects \"economic,\" \"crisis,\" and \"policy.\"  \n",
    "5. **Topic_5**: *Pediatric COVID-19 Outcomes* (Model 1)  \n",
    "   - Concise and directly links COVID-19 to pediatric outcomes.  \n",
    "6. **Topic_6**: *Molecular Therapeutics and Immunomodulation* (Model 1)  \n",
    "   - Balances molecular mechanisms and immune targeting.  \n",
    "7. **Topic_7**: *Thoracic Oncology and Surgical Outcomes* (Model 1)  \n",
    "   - Combines cancer focus with surgical complications.  \n",
    "8. **Topic_8**: *AI Applications in Medical Imaging* (Model 1)  \n",
    "   - Clear and aligns with \"learning,\" \"image,\" and \"tool.\"  \n",
    "\n",
    "---\n",
    "\n",
    "### **Suggested Label Revisions**  \n",
    "- **Topic_4**: All models initially included \"health,\" which isn’t in the word list. Model 3’s label is correct but could be shortened to *Global Economic Crisis Policy*.  \n",
    "- **Topic_6**: Model 3’s label (*Immune Targeted Therapeutic Protein Expression*) is too jargon-heavy; Model 1’s version is preferable.  \n",
    "- **Topic_8**: Model 3’s label (*Image-Based Learning Performance Prediction Tool*) could be simplified to *AI-Driven Medical Imaging Tools*.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Final Scores**  \n",
    "| Metric          | Model 1 | Model 2 | Model 3 |  \n",
    "|-----------------|---------|---------|---------|  \n",
    "| Fluency/Coherence | 4.8     | 4.6     | 3.1     |  \n",
    "| Suitability      | 4.4     | 4.5     | 4.1     |  \n",
    "| Usability        | 4.8     | 4.5     | 3.0     |  \n",
    "\n",
    "**Overall Best Model**: **Model 1** for fluency and usability, though Model 2 is close in suitability. Model 3 excels in specificity but suffers in readability.\n",
    "\n",
    "\n",
    "## Reasoning:\n",
    "```\n",
    "Okay, let's tackle this evaluation step by step. First, I need to understand the task. We have three models that generated topic labels for eight lists of words related to medical/scientific articles. The goal is to evaluate each model's labels based on three metrics: Fluency/Coherence, Suitability, and Usability, each on a 1-5 scale. Then, choose the best label for each topic and note any changes needed.\n",
    "\n",
    "Starting with Topic_1 words: student, participant, survey, mental, online, anxiety, education, child, self, psychological. The key themes here are mental health in an educational context involving children. Model 1's label is \"Mental Health in Educational Settings\" – that's fluent and covers education and mental health, but maybe misses the survey/online aspects. Model 2's \"Pediatric Mental Health Assessment\" includes children and assessment (which ties to survey), more specific. Model 3's \"Child Mental Health Education Survey\" is specific but a bit clunky. Fluency for Model 3 might be lower because it's a longer phrase. Suitability: Model 2 might be better as it includes assessment (survey) and pediatric (child). Usability: Model 1 is more concise but maybe less specific. So maybe Model 2's label is best here.\n",
    "\n",
    "Moving to Topic_2: vaccine, vaccination, antibody, variant, transmission, influenza, testing, assay, detection, detect. The focus is on vaccines, testing, detection, variants, influenza. Model 1: \"Vaccine Immunology and Detection\" – covers immunology (antibodies) and detection, but maybe not testing/assay specifically. Model 2: \"Vaccine Immunology & Surveillance\" – surveillance could relate to monitoring variants and transmission. Model 3: \"Influenza Vaccine Detection Assay\" – very specific to influenza and detection/assay, but maybe too narrow since the words include variant and transmission. Model 1 and 2 are broader. Model 2's \"Surveillance\" might cover transmission tracking. Suitability: Model 2 better? But maybe \"Detection\" from Model 1 is also good. The word \"assay\" is in Model 3's label, which is part of the words. However, Model 3's label is less fluent and too specific. So Model 2 might be better, but I need to check if surveillance fits. Alternatively, Model 1's label might miss testing/assay. Maybe Model 2's \"Surveillance\" includes testing and detection. So Model 2 could be better. But I need to see if \"Surveillance\" is the right term here. Alternatively, Model 1's \"Detection\" is more accurate. Hmm.\n",
    "\n",
    "For Topic_3: mask, food, air, concentration, surface, energy, water, temperature, material, environmental. The words are about environmental factors and possibly pollution, air/water quality. Model 1: \"Environmental Exposure and Contamination\" – contamination relates to air/water, exposure to environmental factors. Model 2: \"Environmental Exposure Determinants\" – determinants might refer to factors affecting exposure, but \"determinants\" is a bit abstract. Model 3: \"Environmental Air Quality Assessment\" – focuses on air quality, but the words include water, food, etc. So Model 3's label is too narrow. Model 1's \"Contamination\" covers air, water, surfaces. But maybe \"Exposure\" is about contact with these elements. So Model 1 is better here. Model 2's \"Determinants\" is less clear. So best label is Model 1.\n",
    "\n",
    "Topic_4: policy, article, service, community, crisis, economic, global, management, paper, world. Themes are global policy, crisis management, economic aspects. Model 1: \"Global Health Policy and Systems\" – but the words don't specifically mention health. Words include economic, crisis, management. Model 2: \"Global Health Policy Management\" – again, health isn't in the words. Model 3: \"Global Economic Crisis Management Policy\" – this includes economic crisis, management, policy, global. That fits better. The original words have economic and crisis, so Model 3's label is more suitable. Model 1 and 2 incorrectly assume health, which isn't in the words. So Model 3 is best here.\n",
    "\n",
    "Topic_5: ci, mortality, p, death, trial, child, admission, severity, cohort, covid. This seems to be about COVID-19 outcomes in children, mortality, severity, clinical trials. Model 1: \"Pediatric COVID-19 Outcomes\" – concise and covers child (pediatric), COVID, outcomes (mortality, severity). Model 2: \"Clinical Outcomes Epidemiology\" – too vague, doesn't mention COVID or pediatric. Model 3: \"COVID-19 Child Mortality Severity Cohort\" – accurate but wordy and not very fluent. Model 1 is better here, more fluent and suitable.\n",
    "\n",
    "Topic_6: protein, drug, immune, target, gene, expression, receptor, mechanism, induce, therapeutic. Focus on drug development, immune system targeting, gene expression. Model 1: \"Molecular Therapeutics and Immunomodulation\" – covers therapeutics and immune modulation. Model 2: \"Molecular Immunotherapeutics\" – similar but shorter. Model 3: \"Immune Targeted Therapeutic Protein Expression\" – a bit jumbled but includes target, immune, therapeutic, protein. Model 1 and 2 are more fluent. Suitability: Maybe Model 2's \"Immunotherapeutics\" captures the immune and drug aspects. But \"Molecular\" might cover gene/protein. Model 1's \"Immunomodulation\" relates to mechanisms. Both are good, but Model 2 is more concise. However, the words include gene expression and receptors, which might be part of molecular mechanisms. So maybe Model 1 is better. Alternatively, Model 3 includes \"protein expression\" which is part of the words, but the label is less fluent. So Model 1 or 2. Model 1's \"Immunomodulation\" might be more specific to immune mechanisms. So maybe Model 1.\n",
    "\n",
    "Topic_7: cancer, surgery, lung, complication, surgical, pain, procedure, undergo, diagnosis, pulmonary. Thoracic surgery related to lung cancer, complications. Model 1: \"Thoracic Oncology and Surgical Outcomes\" – oncology covers cancer, surgical outcomes include complications. Model 2: \"Thoracic Surgical Oncology\" – combines surgery and oncology. Model 3: \"Pulmonary Cancer Surgery Diagnosis Complication\" – includes all elements but is a string of terms. Model 1 and 2 are more fluent. Model 1's \"Surgical Outcomes\" covers complications, pain. Model 2 is more concise. Both are good, but maybe Model 1 is slightly better as it includes outcomes.\n",
    "\n",
    "Topic_8: propose, network, performance, learning, image, application, accuracy, user, predict, tool. This is about AI/ML applications, particularly imaging. Model 1: \"AI Applications in Medical Imaging\" – concise, covers AI, imaging. Model 2: \"Predictive Diagnostic Algorithms\" – predictive relates to predict, diagnostic maybe not directly in words. Model 3: \"Image-Based Learning Performance Prediction Tool\" – includes image, learning, predict, tool, but is a bit wordy. Model 1's label is more usable and fluent. Model 2's label is narrower. Model 1 better suits the words, which include learning (as in machine learning), image, application, etc.\n",
    "\n",
    "Now, going back to each model's labels and scoring them.\n",
    "\n",
    "For Model 1:\n",
    "\n",
    "- Topic_1: Fluency 5, Suitable 4 (misses survey/online), Usability 5.\n",
    "\n",
    "- Topic_2: Fluency 5, Suitable 4 (maybe missing testing/assay), Usability 5.\n",
    "\n",
    "- Topic_3: Fluency 5, Suitable 5, Usability 5.\n",
    "\n",
    "- Topic_4: Fluency 5, Suitable 3 (wrongly includes health), Usability 3.\n",
    "\n",
    "- Topic_5: Fluency 5, Suitable 5, Usability 5.\n",
    "\n",
    "- Topic_6: Fluency 5, Suitable 5, Usability 5.\n",
    "\n",
    "- Topic_7: Fluency 5, Suitable 5, Usability 5.\n",
    "\n",
    "- Topic_8: Fluency 5, Suitable 5, Usability 5.\n",
    "\n",
    "Average scores would be high except for Topic_4.\n",
    "\n",
    "Model 2:\n",
    "\n",
    "- Topic_1: Fluency 5, Suitable 5 (includes pediatric and assessment), Usability 5.\n",
    "\n",
    "- Topic_2: Fluency 5 (though \"&\" might be less formal), Suitable 5 (surveillance covers testing/detection), Usability 5.\n",
    "\n",
    "- Topic_3: Fluency 4 (\"Determinants\" is a bit academic), Suitable 4 (less direct than Model 1), Usability 4.\n",
    "\n",
    "- Topic_4: Fluency 5, Suitable 3 (still includes health), Usability 3.\n",
    "\n",
    "- Topic_5: Fluency 5, Suitable 2 (too vague), Usability 2.\n",
    "\n",
    "- Topic_6: Fluency 5, Suitable 5, Usability 5.\n",
    "\n",
    "- Topic_7: Fluency 5, Suitable 5, Usability 5.\n",
    "\n",
    "- Topic_8: Fluency 5, Suitable 4 (predictive and diagnostic, but maybe narrow), Usability 4.\n",
    "\n",
    "Average might be higher than Model 1 except for Topic_4 and 5.\n",
    "\n",
    "Model 3:\n",
    "\n",
    "- Topic_1: Fluency 3 (clunky), Suitable 5 (covers child, mental health, education, survey), Usability 3.\n",
    "\n",
    "- Topic_2: Fluency 3 (awkward), Suitable 3 (too narrow to influenza), Usability 2.\n",
    "\n",
    "- Topic_3: Fluency 4, Suitable 3 (narrow to air quality), Usability 3.\n",
    "\n",
    "- Topic_4: Fluency 4 (long but correct), Suitable 5, Usability 4 (long but usable).\n",
    "\n",
    "- Topic_5: Fluency 2 (too wordy), Suitable 5, Usability 2.\n",
    "\n",
    "- Topic_6: Fluency 3 (jumbled terms), Suitable 4 (covers aspects but not fluent), Usability 3.\n",
    "\n",
    "- Topic_7: Fluency 2 (string of terms), Suitable 4, Usability 2.\n",
    "\n",
    "- Topic_8: Fluency 3 (wordy), Suitable 4, Usability 3.\n",
    "\n",
    "Scores vary, with some high suitability but lower fluency and usability.\n",
    "\n",
    "Now, compiling all this into scores and explanations, choosing best labels per topic.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9GTBo4VZN8s"
   },
   "source": [
    "#DeepSeek Response - BERTopic\n",
    "### Evaluation of Models  \n",
    "**Scoring Metrics (1-5):**  \n",
    "1. **Fluency/Coherence:** Grammar, readability, and linguistic clarity.  \n",
    "2. **Suitability:** Relevance to the associated word list.  \n",
    "3. **Usability:** Practicality as a concise, informative topic tag.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Model 1**  \n",
    "- **Fluency/Coherence:** 5/5  \n",
    "  - All labels are grammatically flawless and concise.  \n",
    "- **Suitability:** 4/5  \n",
    "  - Generally accurate but occasionally overbroad (e.g., \"Global Health Policy\" for Topic 5 ignores non-health governance terms).  \n",
    "- **Usability:** 5/5  \n",
    "  - Labels are short and intuitive for topic tagging.  \n",
    "\n",
    "**Best Labels:**  \n",
    "- **Topic 7:** *Venous Thromboembolism Management* (specific and precise).  \n",
    "\n",
    "**Suggested Changes:**  \n",
    "- **Topic 5:** Replace \"Global Health Policy\" with \"Global Governance and Law\" (original words lack a health focus).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Model 2**  \n",
    "- **Fluency/Coherence:** 5/5  \n",
    "  - Professionally structured and error-free.  \n",
    "- **Suitability:** 3/5  \n",
    "  - Some labels misalign with word lists (e.g., \"Drug Discovery Proteomics\" for Topic 6 introduces \"proteomics,\" which is absent from the original words).  \n",
    "- **Usability:** 5/5  \n",
    "  - Clear and functional for tagging.  \n",
    "\n",
    "**Best Labels:**  \n",
    "- None outperformed other models.  \n",
    "\n",
    "**Suggested Changes:**  \n",
    "- **Topic 6:** Replace \"Drug Discovery Proteomics\" with \"Protease Inhibitor Design\" (aligns with molecular terms like *mpro*, *ligand*).  \n",
    "\n",
    "---\n",
    "\n",
    "#### **Model 3**  \n",
    "- **Fluency/Coherence:** 5/5  \n",
    "  - Technical but grammatically sound.  \n",
    "- **Suitability:** 5/5  \n",
    "  - Most precise alignment with word lists (e.g., *Pediatric Multisystem Inflammatory Syndrome* for Topic 2 includes \"multisystem,\" a key term).  \n",
    "- **Usability:** 4/5  \n",
    "  - Slightly verbose in some cases (e.g., *Deep Learning Chest X-ray Segmentation* could omit \"Chest X-ray\" for brevity).  \n",
    "\n",
    "**Best Labels (6/8 Topics):**  \n",
    "- **Topic 2:** *Pediatric Multisystem Inflammatory Syndrome*  \n",
    "- **Topic 3:** *Viral Glycan Interactions*  \n",
    "- **Topic 4:** *Deep Learning Chest X-ray Segmentation*  \n",
    "- **Topic 5:** *Global Governance and International Law*  \n",
    "- **Topic 6:** *Protease Inhibitor Compound Design*  \n",
    "- **Topic 8:** *Emergency Healthcare Preparedness and Staffing*  \n",
    "\n",
    "**Suggested Changes:**  \n",
    "- **Topic 1:** Replace *Educational Technology* with *Online Education and Learning* (directly reflects \"online\" and \"student\").  \n",
    "- **Topic 7:** *Venous Thrombosis and Coagulation Management* could merge with Model 1’s *Venous Thromboembolism Management* for conciseness.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Final Scores**  \n",
    "| Model | Fluency/Coherence | Suitability | Usability |  \n",
    "|-------|--------------------|-------------|-----------|  \n",
    "| 1     | 5                  | 4           | 5         |  \n",
    "| 2     | 5                  | 3           | 5         |  \n",
    "| 3     | 5                  | 5           | 4         |  \n",
    "\n",
    "**Best Overall Performer:** **Model 3** (highest suitability despite minor verbosity).  \n",
    "\n",
    "**Key Takeaways:**  \n",
    "- **Model 3** excels in specificity but occasionally sacrifices brevity.  \n",
    "- **Model 1** is strong in usability but less precise.  \n",
    "- **Model 2** struggles with relevance in niche topics.\n",
    "\n",
    "##Reasoning\n",
    "```\n",
    "Okay, let's tackle this evaluation. So, the user wants me to assess three models that generated topic labels for eight medical/scientific article topics based on word lists. The metrics are Fluency/Coherence, Suitability, and Usability, each on a 1-5 scale. Plus, I need to pick the best label for each topic and suggest any changes.\n",
    "\n",
    "First, I should go through each topic one by one. Let me start with Topic_1. The words are about teaching, learning, students, online, education, etc. Model 1 says \"Health Professions Education.\" Hmm, that's specific to health professions, but the original words don't mention health. Model 2: \"Medical Education Pedagogy.\" Again, medical? The original list is general education. Model 3: \"Educational Technology.\" The word 'online' is there, so maybe that's better. Suitability-wise, Models 1 and 2 might be off. Model 3's label fits better with online and educational tech. So Fluency for Model 3 is good, Suitability too. Maybe Model 3 is best here, but could it be better? Maybe \"Online Education and Learning\" to include more keywords.\n",
    "\n",
    "Next, Topic_2: child, pediatric, multisystem, inflammatory, PIM, syndrome, infant, etc. All models have similar labels. Model 1: \"Pediatric Inflammatory Syndromes.\" Model 2 same. Model 3: \"Pediatric Multisystem Inflammatory Syndrome.\" The word 'multisystem' is in the list, so Model 3 is more precise. \"Syndrome\" is singular in the list (PIM, TS), so plural vs. singular. But maybe PIMS is a known term. So Model 3's label is more accurate. Fluency is good, maybe a 5. Suitability higher. So Model 3's label is best here.\n",
    "\n",
    "Topic_3: replication, glycan, protein, RNA, virus, etc. Model 1: \"Viral Molecular Biology.\" That's broad. Model 2 same. Model 3: \"Viral Glycan Interactions.\" Glycan is in the words, so that's specific. Suitability for Model 3 is better because it mentions glycan. But does it cover replication, RNA? Maybe not. So trade-off between specificity and breadth. Maybe Model 3 is better but could be adjusted. Alternatively, \"Viral Replication and Glycosylation Processes\" to include more terms. But as per the given labels, Model 3's is more specific. So higher suitability.\n",
    "\n",
    "Topic_4: image, ray, deep, segmentation, chest, etc. Model 1: \"Medical Imaging and AI.\" AI is part of deep learning. Model 2: \"Medical Imaging Diagnostics.\" Model 3: \"Deep Learning Chest X-ray Segmentation.\" Very specific. The words include 'deep', 'segmentation', 'chest', 'convolutional', 'CT', 'learning'. So Model 3's label is very precise. Suitability high. Fluency is a bit long but still correct. So Model 3's label is best here.\n",
    "\n",
    "Topic_5: government, governance, law, policy, country, etc. Models 1 and 2 mention health or health policy. But the words are general: government, law, public, global, international. Model 3: \"Global Governance and International Law.\" That fits better. Models 1 and 2 incorrectly include 'health' which isn't in the words. So Model 3's label is more suitable here.\n",
    "\n",
    "Topic_6: protease, docking, compound, inhibitor, mpro, drug, etc. Model 1: \"Antiviral Drug Discovery.\" Model 2: \"Drug Discovery Proteomics.\" Model 3: \"Protease Inhibitor Compound Design.\" The words are about protease inhibitors, molecular docking, ligands. Model 3's label is very specific. Suitability is higher. Model 1 and 2 are broader. So Model 3's label is best here.\n",
    "\n",
    "Topic_7: VTE, thrombosis, anticoagulation, embolism, etc. Model 1: \"Venous Thromboembolism Management.\" Model 2: \"Thrombotic Disease Management.\" Model 3: \"Venous Thrombosis and Coagulation Management.\" The original terms include VTE, venous, coagulation. Model 1 is precise with VTE management. Model 3 includes coagulation. Both are good. Maybe Model 1 is more concise and accurate. So best label is Model 1 here. But Model 3's is also good. Need to check which terms are covered.\n",
    "\n",
    "Topic_8: hospital, hcw, bed, ambulance, staff, emergency, etc. Model 1: \"Hospital-Based Emergency Care.\" Model 2: \"Healthcare Facility Operations.\" Model 3: \"Emergency Healthcare Preparedness and Staffing.\" The words include preparedness, staff, ambulance, emergency. Model 3's label includes preparedness and staffing, which are in the list. Model 1 focuses on emergency care. Model 2 is broader. So Model 3's label might be more suitable. But \"Emergency Healthcare Preparedness and Staffing\" is a bit long. Maybe \"Emergency Healthcare Preparedness and Operations\" to cover more. But as per given options, Model 3's label is better.\n",
    "\n",
    "Now, for each model, scoring each metric. Let's go model by model.\n",
    "\n",
    "Model 1:\n",
    "\n",
    "- Topic 1: Health Professions Education. Fluency: 5, Suitability: 3 (since it's health-specific when words aren't), Usability: 4.\n",
    "\n",
    "- Topic 2: Pediatric Inflammatory Syndromes. Fluency 5, Suitability 4 (missing multisystem), Usability 5.\n",
    "\n",
    "- Topic 3: Viral Molecular Biology. Fluency 5, Suitability 4 (broad), Usability 5.\n",
    "\n",
    "- Topic 4: Medical Imaging and AI. Fluency 5, Suitability 4 (covers AI but maybe not all terms), Usability 5.\n",
    "\n",
    "- Topic 5: Global Health Policy. Suitability 3 (health not in words), others 5.\n",
    "\n",
    "- Topic 6: Antiviral Drug Discovery. Suitability 4 (antiviral is inferred), Usability 5.\n",
    "\n",
    "- Topic 7: Venous Thromboembolism Management. All 5s.\n",
    "\n",
    "- Topic 8: Hospital-Based Emergency Care. Suitability 4 (misses preparedness, staffing), Usability 5.\n",
    "\n",
    "Model 1 average would be around 4.5 for Fluency, maybe 4 for Suitability, 4.8 Usability.\n",
    "\n",
    "Model 2:\n",
    "\n",
    "- Topic 1: Medical Education Pedagogy. Again health/medical, same issue. Suitability 3.\n",
    "\n",
    "- Topic 2: Same as Model 1, same scores.\n",
    "\n",
    "- Topic 3: Same as Model 1.\n",
    "\n",
    "- Topic 4: Medical Imaging Diagnostics. Suitable, but missing AI/deep learning. Suitability 4.\n",
    "\n",
    "- Topic 5: Health Policy Governance. Again health, suitability 3.\n",
    "\n",
    "- Topic 6: Drug Discovery Proteomics. Proteomics isn't in the words. The words are about protease inhibitors and molecular docking. So suitability lower, maybe 3.\n",
    "\n",
    "- Topic 7: Thrombotic Disease Management. Broad, but VTE is specific. Suitability 4.\n",
    "\n",
    "- Topic 8: Healthcare Facility Operations. Suitability 5? It covers hospital, facility, staff, etc. Maybe 4.5.\n",
    "\n",
    "Model 2 might have lower suitability in some topics.\n",
    "\n",
    "Model 3:\n",
    "\n",
    "- Topic1: Educational Technology. Suitable, but maybe missing some aspects. Suitability 4.\n",
    "\n",
    "- Topic2: Pediatric Multisystem Inflammatory Syndrome. Suitability 5.\n",
    "\n",
    "- Topic3: Viral Glycan Interactions. Specific but misses replication. Suitability 4.\n",
    "\n",
    "- Topic4: Deep Learning Chest X-ray Segmentation. Very specific, maybe misses CT and other imaging. Suitability 4.\n",
    "\n",
    "- Topic5: Global Governance and International Law. Suitability 5.\n",
    "\n",
    "- Topic6: Protease Inhibitor Compound Design. Suitability 5.\n",
    "\n",
    "- Topic7: Venous Thrombosis and Coagulation Management. Suitability 5.\n",
    "\n",
    "- Topic8: Emergency Healthcare Preparedness and Staffing. Suitability 5.\n",
    "\n",
    "Fluency for Model3: Topic3's label is a bit technical but still fluent. All should be 5. Suitability varies but higher overall. Usability might be high except for longer labels, but still usable.\n",
    "\n",
    "Comparing all, Model3 likely has higher scores across the board, especially in suitability.\n",
    "\n",
    "Now, picking the best label for each topic:\n",
    "\n",
    "Topic1: Model3's \"Educational Technology\" but maybe better as \"Online Education and Learning Technology\"? Since the user allows suggesting changes. But given the options, Model3 is best.\n",
    "\n",
    "Topic2: Model3's label.\n",
    "\n",
    "Topic3: Model3's.\n",
    "\n",
    "Topic4: Model3's.\n",
    "\n",
    "Topic5: Model3's.\n",
    "\n",
    "Topic6: Model3's.\n",
    "\n",
    "Topic7: Model1's \"Venous Thromboembolism Management\" is concise, but Model3's includes coagulation. However, VTE is a specific term encompassing DVT and PE, and the words include coagulation, so maybe Model3's is better. Or maybe Model1 is better. Hmm. The terms include VTE, thrombosis, embolism, coagulation. So Model3's label covers more terms. So best is Model3.\n",
    "\n",
    "Topic8: Model3's label.\n",
    "\n",
    "So overall, Model3 has the best labels for most topics.\n",
    "\n",
    "For changes: Topic1 could be more precise. Maybe \"Online Education and Learning\" instead of \"Educational Technology.\" Topic3 could include replication or RNA aspects. Topic4 could mention CT or other imaging modalities. But given the labels, Model3's are best.\n",
    "\n",
    "Now, scoring each model's metrics:\n",
    "\n",
    "Model1:\n",
    "\n",
    "Fluency: All labels are fluent. 5.\n",
    "\n",
    "Suitability: Some labels are off (Topics1,5). Average around 4.\n",
    "\n",
    "Usability: Concise, usable. 5.\n",
    "\n",
    "Model2:\n",
    "\n",
    "Fluency: 5.\n",
    "\n",
    "Suitability: Issues in Topics1,5,6. Lower, maybe 3.5.\n",
    "\n",
    "Usability: Still good, 5.\n",
    "\n",
    "Model3:\n",
    "\n",
    "Fluency: 5.\n",
    "\n",
    "Suitability: Mostly 5, some 4. Average 4.5-5.\n",
    "\n",
    "Usability: Some labels are longer but still usable. 5.\n",
    "\n",
    "So final scores:\n",
    "\n",
    "Model1: F:5, S:4, U:5\n",
    "\n",
    "Model2: F:5, S:3.5, U:5\n",
    "\n",
    "Model3: F:5, S:5, U:5\n",
    "\n",
    "But since the user wants per-model scores on 1-5, probably:\n",
    "\n",
    "Model1: Fluency 5, Suitability 4, Usability 5\n",
    "\n",
    "Model2: Fluency 5, Suitability 3, Usability 5\n",
    "\n",
    "Model3: Fluency 5, Suitability 5, Usability 5\n",
    "\n",
    "Wait, but for some topics, Model3's suitability may not be perfect. For example, Topic3 misses replication, Topic4 misses CT. So maybe Suitability is 4.5, but since we need to give integer scores, maybe 5 for Model3 as it's the best available. Alternatively, 4 for some. Need to check each model's labels.\n",
    "\n",
    "But according to the user's instruction, evaluate each model's performance using the metrics on 1-5 scale. So per-model, not per-topic. So average across all topics.\n",
    "\n",
    "But maybe the user wants an overall score per model for each metric. So for each model, across all 8 topics, average their Fluency, Suitability, Usability.\n",
    "\n",
    "But perhaps it's better to give an overall score for each model's metrics. Let me think.\n",
    "\n",
    "Alternatively, maybe the user wants each model to be scored on each metric as a whole. For example, Model1's Fluency: how fluent are all its labels? If all are grammatically correct, Fluency is 5. Suitability: average how well each label fits. If some are off, maybe 4. Usability: all are usable tags, maybe 5.\n",
    "\n",
    "Similarly for other models.\n",
    "\n",
    "So:\n",
    "\n",
    "Model1 Fluency: 5 (all labels are fluent)\n",
    "\n",
    "Suitability: Let's see. Topic1 is off (health focus), Topic5 is off (health), others are good. So 2/8 topics have lower suitability. That's 75% good, so maybe 4.\n",
    "\n",
    "Usability: All labels are concise and usable. 5.\n",
    "\n",
    "Model2 Fluency: 5. Suitability: Topic1 (medical), Topic5 (health), Topic6 (proteomics not relevant), and maybe others. 3 topics problematic. So suitability around 3. Usability 5.\n",
    "\n",
    "Model3 Fluency: 5. Suitability: Only perhaps Topic3 and Topic4 could be slightly off, but still much better. Maybe 5. Usability: Some labels longer but still ok. 5.\n",
    "\n",
    "So the scores would be:\n",
    "\n",
    "Model1: F5, S4, U5\n",
    "\n",
    "Model2: F5, S3, U5\n",
    "\n",
    "Model3: F5, S5, U5\n",
    "\n",
    "Then, the best labels per topic are mostly from Model3, except maybe Topic7.\n",
    "\n",
    "But need to check each topic's best label.\n",
    "\n",
    "After finalizing, also mention if any labels should be changed. For example, Topic1: Model3's \"Educational Technology\" could be improved to include online learning. Topic3: Maybe mention replication or viral structure. But given the options, stick with the models' labels.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrZw7JfmZpdB"
   },
   "source": [
    "# Llama3.2-3B-Instruct\n",
    "Below is the implementation for Llama3.2-3B-Instruct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zf4XWVdKzsr"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from transformers import pipeline, logging\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Disable warnings about model loading\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Log in to your Hugging Face account NOTE: you need to use your own Access token\n",
    "login(token=\"YOUR_ACCESS_TOKEN_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575,
     "referenced_widgets": [
      "264e93b82bef4197aad55dcfd742a9ff",
      "1b239411b54143cf854525032e0d45a0",
      "d3e6c73644784b13afd24bd127a70b6e",
      "702b045f7ef148b9a420c1e91aea1189",
      "4b6d024065364842b97b38321c1f97e4",
      "db45bf51596e4b97a2d275cb1f0ca13f",
      "c3471e4743af49999584e61db34b9977",
      "63b71a03a66449cf9fb83b6fe424e202",
      "1a7692f6f2e645a0bd24e07240e54550",
      "8e52980c54ac49218870a44da418bfc8",
      "b4860acfdc8a4ebc80f7b99d6748ed5e",
      "8c8d86f228dc408eaa4367f3d042374f",
      "89f02f2542ea4b88bee6ebd09ab6a50d",
      "0c57a0d48c81487380daa10f15084881",
      "8507e33294dd4383b2dd6317c4878057",
      "0e016734f4e7494484924c1fa2fdc857",
      "7516d050ae034f479b2ffafb6e49cccb",
      "677c8d7e21f246668eb0e8fe830003c0",
      "26ddd0ff6d98481b87f0bc15922c04f2",
      "cee9102e9f2349bcb0fc66a9b49bb452",
      "f8928ad97fea41db9c03bf2a05f41bc0",
      "d55a513e85e54d8ea9f5600b2b0639f9",
      "17c8067cebbc442281fc26997be8998a",
      "45ebc086a5f447d4925bccc55abb7dc4",
      "d05d4d296e184b68bd7948f0e0269f46",
      "83391bfa489b4d4186fb54928fbc9ea9",
      "0acc8f3a32a24331a8a377794eea4334",
      "5cf61afe1ef04ad3b1622ba29ffbb14b",
      "3ecb1933472b4357ad5dd8f7f5af3401",
      "d9decffbed8d448ca9c2472dcba1b03f",
      "9f309e3e6ce4434792e158b9c677827d",
      "798a1a527c3f4b43aaa7686cab2f4345",
      "eac65ba9cc444a1e81d28b44083b112a",
      "3572fdc953184eb985bc4e90e51ebbff",
      "460afef1c6d34618a366590e2a41f989",
      "6ab28abe14c5427884a9c60f62f7b291",
      "138494a0fd2f49cba65d85ec4ff60f00",
      "eb03106636504e498a5de58923533ef8",
      "41395c601cf54476b76be380a7063ee7",
      "febe40a0138c4db1b9018f19dea58026",
      "f272cc6542f84046a7a1b01e147fab11",
      "7e1abc6c206d4f37a14932b4e46ba6aa",
      "6d4f729a23064a469e8f2161bf3daf92",
      "af934f761d41491489854e54dc5f7b9b",
      "71b4b58724b547eb9f6f86ed901b7ae2",
      "0e53de48902c4d01967525f7813669b4",
      "bed10749a34f417c80eefda431dee9aa",
      "2cdf7dd552aa460ab47d76f3d74be573",
      "25340876f8c44bef908c99cf8c763b19",
      "81c252a6266046b5ae3b5c47ca6fd74a",
      "87d76619ff4c458baee233afa007dc74",
      "7cd7fd678a2a4a3883c3a96ad37740c6",
      "159bea001d124eb5b06a0d82539b92df",
      "e35b8c6cdb144dab93ce75f24ebcecae",
      "594d23e17e6e4c18824875464b1fa14b",
      "a10c4b8b81524053a25bc24ac564749e",
      "e88e451e0af14ba5839d4761a7046593",
      "be7349a3b9d94348946d2ef6cc325392",
      "fa031aade32d42bf82c998aaeb64af34",
      "a8865d7ed6f64b318d1d96efbb6cb70a",
      "86e0dfa8588c495eab41a950a350b0ce",
      "22ff6843e16d4b79b0ae2235833039b5",
      "c4e5722be6b447c4966424b895a209af",
      "fadb9a1c22f04fa7adc134fa62acab41",
      "e6119019772b46b98523575dc25c9558",
      "ba261588981f41c7aa758e053ee05135",
      "9c210987cdbf4dafa80fee81e342d507",
      "a9ec77c3eb1e4fc7aaff2fda4ae01bfd",
      "a6f7e0f7d0754c858f98aaf1ac3276fe",
      "71247149d762487e9143b35b44b4648b",
      "e44e4da55fb44703888de2ab0d8c86df",
      "c43371ca4917422dbe109c885eca0caa",
      "bf52716d626d4e31adc0d5c4cfee477b",
      "597fcec92b754980893258df8570a2bb",
      "d5f1d700a72a4885b2d47e059baf453e",
      "6e1ace45650946178c0965d7ef69f789",
      "061824e1b1b74346832f94cd5c13a7b2",
      "22a3e558aa26495a950073158274a5e3",
      "37e50cf04323473b8d4520ef7bfd1282",
      "13abba559b4e4294b9ff91c632562168",
      "689adbf6b9834b1e9710b20c3a4ef5ad",
      "299bca6878d543be8e38afdca0078c04",
      "8b70e0951a9a4cdd81aa7ecb785f5cd1",
      "ce5b839d7e024513b34c70915b2bba2c",
      "d089b0e636ff47919976c6d6e0dbbb04",
      "db724792afd54342a7ffe8c87bbcb1c0",
      "1c414ff9f1874dcc97b2ab54a4995e22",
      "faefaf76545348b8bc114c8e004aa969",
      "c5e1ed0329f9434eb20a0fea66f439d4",
      "90dceb36f87c43c39d52b1bdb1759108",
      "2cd56d86eb58471a96490c032928c6ce",
      "56ba96dc9f3d4037a2d070ac14e31b84",
      "f0e6adba7d1c466c9306344931632def",
      "9da55ff164d648bbbc67e9ddd9b60276",
      "0aa20125b16e44eea21a8be746702f16",
      "0220ee3e1dec4faba91c067e000981e5",
      "cb49b73fc29b4e1ebd9835fd3d18677c",
      "c8aaa074559646b0adf6c103e4e6cb84",
      "68264ebdcad04921a17e382c6bdb0157",
      "76dca69a8f9a474eb78ad860cdb5a8f5",
      "8a282618d3be4afa8ea3980304cb6852",
      "88b116dd76c04bf1bbc11b23bb1e59b9",
      "cb1095cd69e2476abedaf908f92d0e6c",
      "a480c24c2f774fa3b4932a767d78eedd",
      "8213950a4f0a45fbbfb26aac88c465d7",
      "c770522cf86e436f8fcfdf90a3cb865a",
      "609b54174fb24af2bd545b961d86801f",
      "acd69bd8e18b47449b16f64c6e67ff6a",
      "f12afd880ef14bc8a718fc89583181f8",
      "93588db4f0914abbbc291b8d6695ec93"
     ]
    },
    "id": "diFv1DaVJedx",
    "outputId": "31b2bedf-2c23-4d01-c5c0-2f799a11d73b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264e93b82bef4197aad55dcfd742a9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8d86f228dc408eaa4367f3d042374f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c8067cebbc442281fc26997be8998a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3572fdc953184eb985bc4e90e51ebbff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b4b58724b547eb9f6f86ed901b7ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10c4b8b81524053a25bc24ac564749e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c210987cdbf4dafa80fee81e342d507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a3e558aa26495a950073158274a5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e1ed0329f9434eb20a0fea66f439d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dca69a8f9a474eb78ad860cdb5a8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the analyzed keyword groups with specific, descriptive labels for each topic:\n",
      "\n",
      "1. Topic_1: Child Mental Health Education Survey\n",
      "2. Topic_2: Influenza Vaccine Detection Assay\n",
      "3. Topic_3: Environmental Air Quality Assessment\n",
      "4. Topic_4: Global Economic Crisis Management Policy\n",
      "5. Topic_5: COVID-19 Child Mortality Severity Cohort\n",
      "6. Topic_6: Immune Targeted Therapeutic Protein Expression\n",
      "7. Topic_7: Pulmonary Cancer Surgery Diagnosis Complication\n",
      "8. Topic_8: Image-Based Learning Performance Prediction Tool\n"
     ]
    }
   ],
   "source": [
    "# Load the model using pipeline\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Create your prompt\n",
    "prompt = \"\"\"Please analyze these keyword groups and create a specific, descriptive label for each topic that:\n",
    "Captures the central theme with precision in the style of the example topics given.\n",
    "Uses domain-specific terminology appropriate for medical/scientific research\n",
    "Is concise (preferably 2-5 words).\n",
    "Format your response as a numbered list corresponding to the topic numbers provided.\n",
    "Example Labels: Cardiology General, Clinical Medicine General, Coronavirus, Critical Care, Diagnostics, Global Health, Hospital-Based Clinical Medicine, Infectious Disease General, Neurology/Neurosurgery General, Outpatient-Based Clinical Medicine, Public Health General, Viral Infections, Pulmonary/Critical Care General\n",
    "\n",
    "\n",
    "Input:\n",
    "\n",
    "Topic_1: student, participant, survey, mental, online, anxiety, education, child, self, psychological\n",
    "Topic_2: vaccine, vaccination, antibody, variant, transmission, influenza, testing, assay, detection, detect\n",
    "Topic_3: mask, food, air, concentration, surface, energy, water, temperature, material, environmental\n",
    "Topic_4: policy, article, service, community, crisis, economic, global, management, paper, world\n",
    "Topic_5: ci, mortality, p, death, trial, child, admission, severity, cohort, covid\n",
    "Topic_6: protein, drug, immune, target, gene, expression, receptor, mechanism, induce, therapeutic\n",
    "Topic_7: cancer, surgery, lung, complication, surgical, pain, procedure, undergo, diagnosis, pulmonary\n",
    "Topic_8: propose, network, performance, learning, image, application, accuracy, user, predict, tool\n",
    "\n",
    "Labels:\"\"\"\n",
    "\n",
    "# Format the prompt for chat models\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# Generate response\n",
    "response = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.5,#this was experiment at 0.5 and 1, but 0.5 was chosen for more control.\n",
    "    do_sample=True,\n",
    "    return_full_text=False\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vRe8IfW4Q55T",
    "outputId": "da9d8fa8-2e16-4e01-bc02-77a801f65b85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the specific, descriptive labels for each topic:\n",
      "\n",
      "1. Topic_1: 'Educational Technology'\n",
      "2. Topic_2: 'Pediatric Multisystem Inflammatory Syndrome'\n",
      "3. Topic_3: 'Viral Glycan Interactions'\n",
      "4. Topic_4: 'Deep Learning Chest X-ray Segmentation'\n",
      "5. Topic_5: 'Global Governance and International Law'\n",
      "6. Topic_6: 'Protease Inhibitor Compound Design'\n",
      "7. Topic_7: 'Venous Thrombosis and Coagulation Management'\n",
      "8. Topic_8: 'Emergency Healthcare Preparedness and Staffing'\n"
     ]
    }
   ],
   "source": [
    "# Create your prompt\n",
    "prompt = \"\"\"Please analyze these keyword groups and create a specific, descriptive label for each topic that:\n",
    "Captures the central theme with precision in the style of the example topics given.\n",
    "Uses domain-specific terminology appropriate for medical/scientific research\n",
    "Is concise (preferably 2-5 words).\n",
    "Format your response as a numbered list corresponding to the topic numbers provided.\n",
    "Example Labels: Cardiology General, Clinical Medicine General, Coronavirus, Critical Care, Diagnostics, Global Health, Hospital-Based Clinical Medicine, Infectious Disease General, Neurology/Neurosurgery General, Outpatient-Based Clinical Medicine, Public Health General, Viral Infections, Pulmonary/Critical Care General\n",
    "\n",
    "\n",
    "Input:\n",
    "\n",
    "Topic_1: ['teacher', 'teaching', 'learning', 'learn', 'student', 'online', 'education', 'university', 'educational', 'learner']\n",
    "Topic_2: ['child', 'pediatric', 'multisystem', 'inflammatory', 'pim', 'syndrome', 'infant', 'adult', 'fever', 'ts']\n",
    "Topic_3: ['replication', 'glycan', 'protein', 'rna', 'virus', 'viral', 'membrane', 'glycosylation', 'virion', 'host']\n",
    "Topic_4: ['image', 'ray', 'deep', 'segmentation', 'chest', 'classification', 'dataset', 'convolutional', 'ct', 'learning']\n",
    "Topic_5: ['government', 'governance', 'law', 'policy', 'country', 'public', 'global', 'right', 'political', 'international']\n",
    "Topic_6: ['protease', 'docking', 'compound', 'inhibitor', 'mpro', 'drug', 'molecular', 'bind', 'ligand', 'pro']\n",
    "Topic_7: ['vte', 'thrombosis', 'anticoagulation', 'embolism', 'venous', 'coagulation', 'thrombotic', 'thromboembolism', 'dimer', 'heparin']\n",
    "Topic_8: ['hospital', 'hcw', 'bed', 'ambulance', 'staff', 'emergency', 'healthcare', 'preparedness', 'care', 'facility']\n",
    "\n",
    "Labels:\"\"\"\n",
    "\n",
    "# Format the prompt for chat models\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "# Generate response\n",
    "response = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.5,\n",
    "    do_sample=True,\n",
    "    return_full_text=False\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
